{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1740rrojbZ0zRzl-cnQ_Tvh2C3OBOZhIi","timestamp":1762207287052}],"authorship_tag":"ABX9TyMAv5umL4SY2o22D8fReCo1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KO-yoC5gohYv","executionInfo":{"status":"ok","timestamp":1762207341363,"user_tz":300,"elapsed":25644,"user":{"displayName":"opeyemi adeniran","userId":"06352095503427961436"}},"outputId":"bf99ac2d-4632-475f-8add-9dcc1b6d1334"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install -q anthropic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KcIJOcbNpd9n","executionInfo":{"status":"ok","timestamp":1762207450987,"user_tz":300,"elapsed":6330,"user":{"displayName":"opeyemi adeniran","userId":"06352095503427961436"}},"outputId":"f6d1517f-a167-4740-a6d5-c571c6e9ee1d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/357.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["#This script tests ALL 8 prompting techniques from your original study:"],"metadata":{"id":"FWM6j-UG8_7i"}},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","COMPREHENSIVE Sensitivity Analysis for ALL Prompting Techniques\n","================================================================================\n","This script tests ALL 8 prompting techniques from your original study:\n","1. Zero-Shot\n","2. Sequential Prompting\n","3. Least-to-Most\n","4. ReAct (Reasoning and Acting)\n","5. Chain of Thought (CoT)\n","6. True Iterative\n","7. Self-Consistency\n","8. Meta-Prompting\n","\n","For EACH technique, tests:\n","- Chunk sizes: 5, 10, 15, 20 frames\n","- Max tokens: 2048, 4096, 8192\n","- Temperatures: 0.1, 0.5, 1.0\n","- Frame extraction: Every 90th frame\n","- All 3 models: GPT-4o, Gemini Pro 1.5, Claude Sonnet 4.5\n","================================================================================\n","\"\"\"\n","\n","import os\n","import json\n","import time\n","import base64\n","import requests\n","from datetime import datetime\n","import anthropic\n","import google.generativeai as genai\n","from openai import OpenAI\n","from PIL import Image\n","import io\n","\n","# ========================== CONFIGURATION ==========================\n","DATA_DIR = \"/content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/COMPLETED/PROMPTS/crime-data\"\n","RESULT_BASE_DIR = \"/content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/COMPLETED/PROMPTS/SENSITIVITY_ANALYSIS_COMPLETE\"\n","\n","# API Key Paths\n","GPT_KEY_PATH = \"/content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/COMPLETED/PROMPTS/SAVE/FINAL-COMPLETED/API-KEYS/chatgpt.txt\"\n","GEMINI_KEY_PATH = \"/content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/COMPLETED/PROMPTS/SAVE/FINAL-COMPLETED/API-KEYS/Gemini.txt\"\n","CLAUDE_KEY_PATH = \"/content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/COMPLETED/PROMPTS/SAVE/FINAL-COMPLETED/API-KEYS/claude.txt\"\n","\n","# Frame extraction\n","FRAME_INTERVAL = 90  # Every 90th frame\n","\n","# Sensitivity parameters\n","CHUNK_SIZES = [5, 10, 15, 20]\n","MAX_TOKENS_OPTIONS = [2048, 4096, 8192]\n","TEMPERATURE_OPTIONS = [0.1, 0.5, 1.0]\n","\n","# Baseline (from original study)\n","BASELINE_CHUNK_SIZE = 10\n","BASELINE_MAX_TOKENS = 4096\n","BASELINE_TEMPERATURE = 0.1\n","\n","# ========================== ALL PROMPTING TECHNIQUES ==========================\n","\n","# 1. ZERO-SHOT PROMPTING\n","ZERO_SHOT_PROMPT = \"What's happening in these frames? Describe the scene and any notable actions or events.\"\n","\n","# 2. SEQUENTIAL PROMPTING (5 steps)\n","SEQUENTIAL_PROMPTS = [\n","    \"What's happening in these frames? Describe the scene at a high level.\",\n","    \"Based on what you observed in your previous response, who are the main people in the scene and what are they doing?\",\n","    \"Looking at the actions you described, do you observe any potential criminal activities? If so, describe them in detail.\",\n","    \"Based on your crime analysis, what objects or items are involved in the incident?\",\n","    \"Considering all your observations, create a chronological timeline of events shown in these frames.\"\n","]\n","\n","# 3. LEAST-TO-MOST PROMPTING (8 steps - simple to complex)\n","LEAST_TO_MOST_PROMPTS = [\n","    \"List all visible objects in these frames. Just identify what you can see - furniture, tools, vehicles, etc.\",\n","    \"Identify the people visible in these frames. How many are there? Describe each person's basic appearance (clothing, general features).\",\n","    \"Describe the location and setting shown in these frames. What kind of place is this? Describe the spatial layout.\",\n","    \"What actions are the people performing in these frames? List simple actions you can observe.\",\n","    \"How are the people interacting with each other and with objects in the scene? Describe specific interactions.\",\n","    \"Do you notice any unusual, concerning, or potentially suspicious behaviors in these frames? If so, what specifically seems unusual?\",\n","    \"Based on your previous observations, analyze whether any potential criminal activities might be occurring. What specific elements suggest criminal behavior?\",\n","    \"Using all your previous observations, construct a detailed chronological timeline of events shown in these frames. Include who did what, when, and potential motives.\"\n","]\n","\n","# 4. REACT PROMPTING (Reasoning and Acting)\n","REACT_PROMPT_TEMPLATE = \"\"\"Analyze these frames using the ReAct approach (Reasoning and Acting). For each important element you observe:\n","\n","1. Thought: Reason about what you're seeing and what it might mean\n","2. Action: Describe what specific aspect you'll focus on analyzing next\n","3. Observation: Make detailed observations about that aspect\n","4. Decision: Draw a conclusion based on your observations\n","\n","Specifically, follow this cycle for:\n","- People and their appearances\n","- Actions and behaviors\n","- Objects and items\n","- Spatial relationships\n","- Temporal sequence of events\n","- Potential criminal activity\n","\n","After going through these cycles, provide your final analysis of what crime appears to be occurring, who is involved, and what evidence supports this conclusion.\n","\n","You are now analyzing frames {frame_range} of {total_frames}.\"\"\"\n","\n","# 5. CHAIN OF THOUGHT PROMPTING\n","COT_PROMPT_TEMPLATE = \"\"\"Analyze these video frames using a chain of thought reasoning process. Think step by step as you examine what's happening:\n","\n","Step 1: First, carefully observe and list what you can actually see in the frames. Note people, objects, settings, and actions without interpretation.\n","\n","Step 2: Identify the key actors in the scene. Describe each person's appearance and what they are doing. Track individuals across multiple frames.\n","\n","Step 3: Describe the sequence of events chronologically. What happens first, next, and after that?\n","\n","Step 4: Note any important objects or items in the scene and how they're being used.\n","\n","Step 5: Consider the context and setting. Where is this taking place? What kind of environment is shown?\n","\n","Step 6: Based on all the above observations, describe what appears to be happening in these frames.\n","\n","Make sure to clearly show your thinking process for each step. These are frames {frame_range} of {total_frames}.\"\"\"\n","\n","# 6. TRUE ITERATIVE PROMPTING\n","TRUE_ITERATIVE_BASE_PROMPT = \"Analyze these frames and describe what you observe about the scene, people, and activities.\"\n","TRUE_ITERATIVE_FOLLOWUPS = [\n","    \"Based on your previous observation, what specific details about the people stand out? Describe their clothing, positioning, and body language.\",\n","    \"Now focusing on the actions: what exactly are these people doing? Be very specific about their movements and interactions.\",\n","    \"Looking at the environment and objects: what items or environmental features are significant? How do they relate to the actions?\",\n","    \"Considering everything you've observed: what appears to be the nature of this incident? What type of activity or crime might be occurring?\",\n","    \"Final synthesis: Provide a comprehensive analysis of what happened in these frames, including timeline, participants, actions, and conclusions.\"\n","]\n","\n","# 7. SELF-CONSISTENCY PROMPTING\n","SELF_CONSISTENCY_PROMPT = \"\"\"Analyze these frames and provide THREE INDEPENDENT analyses from different perspectives:\n","\n","Analysis 1 - Focus on Physical Evidence:\n","Look at the frames focusing purely on physical evidence - objects, locations, visible actions. What concrete evidence do you see?\n","\n","Analysis 2 - Focus on Behavioral Patterns:\n","Look at the frames focusing on human behavior - body language, interactions, movements. What behavioral patterns emerge?\n","\n","Analysis 3 - Focus on Temporal Sequence:\n","Look at the frames focusing on the sequence of events - what happens first, next, last. What is the timeline?\n","\n","After providing all three analyses, synthesize them into a final conclusion about what crime or incident is occurring. Note any agreements or disagreements between the three perspectives.\"\"\"\n","\n","# 8. META-PROMPTING\n","META_PROMPT_TEMPLATE = \"\"\"You are analyzing crime video frames. Before analyzing the actual frames, first think about:\n","\n","1. Strategy Planning: What aspects should you focus on to effectively analyze a potential crime scene?\n","2. Information Priorities: What information is most critical vs. supplementary?\n","3. Analysis Framework: What systematic approach will you use?\n","\n","Now, using the strategy you just developed, analyze these {frame_count} frames:\n","- Apply your planned analysis strategy\n","- Prioritize the information you identified as critical\n","- Follow your systematic framework\n","\n","Frames {frame_range} of {total_frames}.\n","\n","Provide both your strategic thinking AND your frame analysis.\"\"\"\n","\n","# ========================== UTILITY FUNCTIONS ==========================\n","\n","def load_api_key(filepath):\n","    \"\"\"Load API key from file\"\"\"\n","    try:\n","        with open(filepath, 'r') as f:\n","            key = f.read().strip()\n","        print(f\"✓ Loaded API key from {os.path.basename(filepath)}\")\n","        return key\n","    except Exception as e:\n","        print(f\"✗ Failed to load API key from {filepath}: {str(e)}\")\n","        return None\n","\n","def encode_image_base64(image_path):\n","    \"\"\"Encode image to base64\"\"\"\n","    try:\n","        with open(image_path, \"rb\") as image_file:\n","            return base64.b64encode(image_file.read()).decode('utf-8')\n","    except Exception as e:\n","        print(f\"Error encoding image {image_path}: {str(e)}\")\n","        return None\n","\n","def save_results(results, save_dir, filename):\n","    \"\"\"Save results to JSON file\"\"\"\n","    os.makedirs(save_dir, exist_ok=True)\n","    filepath = os.path.join(save_dir, filename)\n","\n","    with open(filepath, 'w') as f:\n","        json.dump(results, f, indent=2)\n","\n","    print(f\"  ✓ Saved: {filename}\")\n","\n","def discover_videos_and_frames(data_dir, frame_interval=90):\n","    \"\"\"Discover all videos and extract every Nth frame\"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(f\"DISCOVERING VIDEOS - EXTRACTING EVERY {frame_interval}TH FRAME\")\n","    print(f\"{'='*80}\")\n","\n","    all_videos = {}\n","    crime_types = [d for d in os.listdir(data_dir)\n","                   if os.path.isdir(os.path.join(data_dir, d))]\n","\n","    print(f\"Found {len(crime_types)} crime types\")\n","\n","    for crime_type in crime_types:\n","        crime_dir = os.path.join(data_dir, crime_type)\n","        all_files = os.listdir(crime_dir)\n","\n","        video_frames = {}\n","        for filename in all_files:\n","            if filename.endswith('.png'):\n","                parts = filename.split('_frame_')\n","                if len(parts) == 2:\n","                    video_name = parts[0]\n","                    frame_num = int(parts[1].replace('.png', ''))\n","\n","                    if video_name not in video_frames:\n","                        video_frames[video_name] = []\n","                    video_frames[video_name].append((frame_num, filename))\n","\n","        for video_name, frames in video_frames.items():\n","            frames.sort(key=lambda x: x[0])\n","            selected_frames = {}\n","\n","            for i in range(0, len(frames), frame_interval):\n","                frame_num, frame_file = frames[i]\n","                frame_path = os.path.join(crime_dir, frame_file)\n","\n","                encoded = encode_image_base64(frame_path)\n","                if encoded:\n","                    selected_frames[frame_file] = encoded\n","\n","            video_key = f\"{crime_type}_{video_name}\"\n","            all_videos[video_key] = {\n","                'crime_type': crime_type,\n","                'video_name': video_name,\n","                'frames': selected_frames,\n","                'total_original_frames': len(frames),\n","                'frames_extracted': len(selected_frames),\n","                'extraction_interval': frame_interval\n","            }\n","\n","            print(f\"  {video_key}: {len(frames)} → {len(selected_frames)} frames\")\n","\n","    print(f\"\\nTotal videos: {len(all_videos)}\")\n","    return all_videos\n","\n","# ========================== MODEL PROCESSORS ==========================\n","\n","class GPTProcessor:\n","    \"\"\"Process frames using GPT-4o\"\"\"\n","    def __init__(self, api_key):\n","        self.api_key = api_key\n","        self.headers = {\n","            \"Authorization\": f\"Bearer {api_key}\",\n","            \"Content-Type\": \"application/json\"\n","        }\n","\n","    def process_chunk(self, prompt, frames, chunk_size, max_tokens, temperature):\n","        \"\"\"Process a chunk of frames\"\"\"\n","        url = \"https://api.openai.com/v1/chat/completions\"\n","\n","        content = [{\"type\": \"text\", \"text\": prompt}]\n","        for frame_base64 in frames:\n","            content.append({\n","                \"type\": \"image_url\",\n","                \"image_url\": {\n","                    \"url\": f\"data:image/png;base64,{frame_base64}\",\n","                    \"detail\": \"high\"\n","                }\n","            })\n","\n","        messages = [{\"role\": \"user\", \"content\": content}]\n","\n","        payload = {\n","            \"model\": \"gpt-4o\",\n","            \"messages\": messages,\n","            \"max_tokens\": max_tokens,\n","            \"temperature\": temperature\n","        }\n","\n","        try:\n","            response = requests.post(url, headers=self.headers, json=payload)\n","\n","            if response.status_code != 200:\n","                return {\"error\": f\"API Error {response.status_code}\"}\n","\n","            result = response.json()\n","            if \"choices\" in result and result[\"choices\"]:\n","                return {\n","                    \"response\": result[\"choices\"][0][\"message\"][\"content\"],\n","                    \"usage\": result.get(\"usage\", {}),\n","                    \"model\": \"gpt-4o\"\n","                }\n","            return {\"error\": \"No response from API\"}\n","\n","        except Exception as e:\n","            return {\"error\": str(e)}\n","\n","class GeminiProcessor:\n","    \"\"\"Process frames using Gemini\"\"\"\n","    def __init__(self, api_key):\n","        genai.configure(api_key=api_key)\n","        self.model = genai.GenerativeModel('gemini-1.5-pro')\n","\n","    def process_chunk(self, prompt, frames, chunk_size, max_tokens, temperature):\n","        \"\"\"Process a chunk of frames\"\"\"\n","        try:\n","            content = [prompt]\n","\n","            for frame_base64 in frames:\n","                frame_bytes = base64.b64decode(frame_base64)\n","                img = Image.open(io.BytesIO(frame_bytes))\n","                content.append(img)\n","\n","            generation_config = {\n","                'max_output_tokens': max_tokens,\n","                'temperature': temperature,\n","            }\n","\n","            response = self.model.generate_content(\n","                content,\n","                generation_config=generation_config\n","            )\n","\n","            return {\n","                \"response\": response.text,\n","                \"model\": \"gemini-1.5-pro\"\n","            }\n","\n","        except Exception as e:\n","            return {\"error\": str(e)}\n","\n","class ClaudeProcessor:\n","    \"\"\"Process frames using Claude\"\"\"\n","    def __init__(self, api_key):\n","        self.client = anthropic.Anthropic(api_key=api_key)\n","\n","    def process_chunk(self, prompt, frames, chunk_size, max_tokens, temperature):\n","        \"\"\"Process a chunk of frames\"\"\"\n","        try:\n","            content = [{\"type\": \"text\", \"text\": prompt}]\n","\n","            for frame_base64 in frames:\n","                content.append({\n","                    \"type\": \"image\",\n","                    \"source\": {\n","                        \"type\": \"base64\",\n","                        \"media_type\": \"image/png\",\n","                        \"data\": frame_base64\n","                    }\n","                })\n","\n","            message = self.client.messages.create(\n","                model=\"claude-sonnet-4-5-20250929\",\n","                max_tokens=max_tokens,\n","                temperature=temperature,\n","                messages=[\n","                    {\"role\": \"user\", \"content\": content}\n","                ]\n","            )\n","\n","            return {\n","                \"response\": message.content[0].text,\n","                \"usage\": {\n","                    \"input_tokens\": message.usage.input_tokens,\n","                    \"output_tokens\": message.usage.output_tokens\n","                },\n","                \"model\": \"claude-sonnet-4.5\"\n","            }\n","\n","        except Exception as e:\n","            return {\"error\": str(e)}\n","\n","# ========================== PROMPTING TECHNIQUE PROCESSORS ==========================\n","\n","def get_prompt_for_technique(technique, chunk_idx, total_chunks, frame_range, total_frames, step=None):\n","    \"\"\"Get the appropriate prompt for each technique\"\"\"\n","\n","    if technique == \"zero_shot\":\n","        return ZERO_SHOT_PROMPT\n","\n","    elif technique == \"sequential\":\n","        if step is None:\n","            step = 0\n","        return SEQUENTIAL_PROMPTS[min(step, len(SEQUENTIAL_PROMPTS)-1)]\n","\n","    elif technique == \"least_to_most\":\n","        if step is None:\n","            step = 0\n","        return LEAST_TO_MOST_PROMPTS[min(step, len(LEAST_TO_MOST_PROMPTS)-1)]\n","\n","    elif technique == \"react\":\n","        return REACT_PROMPT_TEMPLATE.format(\n","            frame_range=frame_range,\n","            total_frames=total_frames\n","        )\n","\n","    elif technique == \"cot\":\n","        return COT_PROMPT_TEMPLATE.format(\n","            frame_range=frame_range,\n","            total_frames=total_frames\n","        )\n","\n","    elif technique == \"true_iterative\":\n","        if step == 0:\n","            return TRUE_ITERATIVE_BASE_PROMPT\n","        else:\n","            return TRUE_ITERATIVE_FOLLOWUPS[min(step-1, len(TRUE_ITERATIVE_FOLLOWUPS)-1)]\n","\n","    elif technique == \"self_consistency\":\n","        return SELF_CONSISTENCY_PROMPT\n","\n","    elif technique == \"meta_prompting\":\n","        frame_count = (chunk_idx + 1) * 10  # Approximate\n","        return META_PROMPT_TEMPLATE.format(\n","            frame_count=frame_count,\n","            frame_range=frame_range,\n","            total_frames=total_frames\n","        )\n","\n","    else:\n","        return ZERO_SHOT_PROMPT\n","\n","# ========================== SENSITIVITY TESTING ==========================\n","\n","def run_technique_sensitivity_test(processor, video_data, technique, test_config):\n","    \"\"\"\n","    Run sensitivity test for a specific prompting technique\n","    \"\"\"\n","    frames_dict = video_data['frames']\n","    frame_names = sorted(frames_dict.keys(),\n","                        key=lambda x: int(x.split('_frame_')[1].replace('.png', '')))\n","    frame_data = [frames_dict[name] for name in frame_names]\n","\n","    chunk_size = test_config['chunk_size']\n","    max_tokens = test_config['max_tokens']\n","    temperature = test_config['temperature']\n","\n","    results = {\n","        'config': test_config,\n","        'technique': technique,\n","        'video_info': {\n","            'crime_type': video_data['crime_type'],\n","            'video_name': video_data['video_name'],\n","            'frames_used': len(frame_data),\n","            'extraction_interval': video_data['extraction_interval']\n","        },\n","        'chunks': [],\n","        'errors': []\n","    }\n","\n","    num_chunks = (len(frame_data) + chunk_size - 1) // chunk_size\n","    total_frames = len(frame_data)\n","\n","    # For multi-step techniques\n","    if technique in [\"sequential\", \"least_to_most\", \"true_iterative\"]:\n","        # Process each step across all chunks\n","        if technique == \"sequential\":\n","            steps = SEQUENTIAL_PROMPTS\n","        elif technique == \"least_to_most\":\n","            steps = LEAST_TO_MOST_PROMPTS\n","        else:  # true_iterative\n","            steps = [TRUE_ITERATIVE_BASE_PROMPT] + TRUE_ITERATIVE_FOLLOWUPS\n","\n","        conversation_history = []\n","\n","        for step_idx, step_prompt in enumerate(steps):\n","            print(f\"      Step {step_idx+1}/{len(steps)}\")\n","\n","            for i in range(0, len(frame_data), chunk_size):\n","                chunk = frame_data[i:i+chunk_size]\n","                chunk_num = i // chunk_size + 1\n","                frame_start = i + 1\n","                frame_end = min(i + chunk_size, total_frames)\n","                frame_range = f\"{frame_start}-{frame_end}\"\n","\n","                prompt = get_prompt_for_technique(\n","                    technique, chunk_num-1, num_chunks,\n","                    frame_range, total_frames, step_idx\n","                )\n","\n","                start_time = time.time()\n","                result = processor.process_chunk(prompt, chunk, chunk_size, max_tokens, temperature)\n","                processing_time = time.time() - start_time\n","\n","                chunk_result = {\n","                    'step': step_idx + 1,\n","                    'chunk_number': chunk_num,\n","                    'frames_in_chunk': len(chunk),\n","                    'processing_time': processing_time,\n","                    'prompt': prompt\n","                }\n","\n","                if 'error' in result:\n","                    chunk_result['error'] = result['error']\n","                    results['errors'].append(f\"Step {step_idx+1} Chunk {chunk_num}: {result['error']}\")\n","                else:\n","                    chunk_result['response'] = result['response']\n","                    if 'usage' in result:\n","                        chunk_result['usage'] = result['usage']\n","                    conversation_history.append(result['response'])\n","\n","                results['chunks'].append(chunk_result)\n","                time.sleep(3)\n","\n","    else:\n","        # Single-pass techniques\n","        for i in range(0, len(frame_data), chunk_size):\n","            chunk = frame_data[i:i+chunk_size]\n","            chunk_num = i // chunk_size + 1\n","            frame_start = i + 1\n","            frame_end = min(i + chunk_size, total_frames)\n","            frame_range = f\"{frame_start}-{frame_end}\"\n","\n","            print(f\"      Chunk {chunk_num}/{num_chunks}\")\n","\n","            prompt = get_prompt_for_technique(\n","                technique, chunk_num-1, num_chunks,\n","                frame_range, total_frames\n","            )\n","\n","            start_time = time.time()\n","            result = processor.process_chunk(prompt, chunk, chunk_size, max_tokens, temperature)\n","            processing_time = time.time() - start_time\n","\n","            chunk_result = {\n","                'chunk_number': chunk_num,\n","                'frames_in_chunk': len(chunk),\n","                'processing_time': processing_time,\n","                'prompt': prompt\n","            }\n","\n","            if 'error' in result:\n","                chunk_result['error'] = result['error']\n","                results['errors'].append(f\"Chunk {chunk_num}: {result['error']}\")\n","            else:\n","                chunk_result['response'] = result['response']\n","                if 'usage' in result:\n","                    chunk_result['usage'] = result['usage']\n","\n","            results['chunks'].append(chunk_result)\n","            time.sleep(3)\n","\n","    return results\n","\n","def run_full_sensitivity_for_model(model_name, processor, videos, save_dir, techniques_to_test=None):\n","    \"\"\"\n","    Run complete sensitivity analysis for a model across ALL techniques\n","    \"\"\"\n","    if techniques_to_test is None:\n","        techniques_to_test = [\n","            \"zero_shot\", \"sequential\", \"least_to_most\", \"react\",\n","            \"cot\", \"true_iterative\", \"self_consistency\", \"meta_prompting\"\n","        ]\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"COMPREHENSIVE SENSITIVITY ANALYSIS FOR {model_name.upper()}\")\n","    print(f\"Testing {len(techniques_to_test)} prompting techniques\")\n","    print(f\"{'='*80}\")\n","\n","    all_results = {\n","        'model': model_name,\n","        'analysis_date': datetime.now().isoformat(),\n","        'frame_interval': FRAME_INTERVAL,\n","        'techniques_tested': techniques_to_test,\n","        'tests': {}\n","    }\n","\n","    test_video_key = list(videos.keys())[0]\n","    test_video = videos[test_video_key]\n","\n","    print(f\"\\nTest video: {test_video_key}\")\n","    print(f\"Frames: {test_video['frames_extracted']} (from {test_video['total_original_frames']})\")\n","\n","    # For each prompting technique\n","    for technique in techniques_to_test:\n","        print(f\"\\n{'-'*80}\")\n","        print(f\"TESTING TECHNIQUE: {technique.upper()}\")\n","        print(f\"{'-'*80}\")\n","\n","        all_results['tests'][technique] = {}\n","\n","        # Test 1: Baseline\n","        print(f\"\\n  Baseline Configuration:\")\n","        print(f\"    Chunk: {BASELINE_CHUNK_SIZE}, Tokens: {BASELINE_MAX_TOKENS}, Temp: {BASELINE_TEMPERATURE}\")\n","\n","        baseline_config = {\n","            'chunk_size': BASELINE_CHUNK_SIZE,\n","            'max_tokens': BASELINE_MAX_TOKENS,\n","            'temperature': BASELINE_TEMPERATURE,\n","            'test_type': 'baseline'\n","        }\n","\n","        all_results['tests'][technique]['baseline'] = run_technique_sensitivity_test(\n","            processor, test_video, technique, baseline_config\n","        )\n","\n","        # Test 2: Chunk Size Sensitivity\n","        print(f\"\\n  Chunk Size Sensitivity: {CHUNK_SIZES}\")\n","        all_results['tests'][technique]['chunk_size_sensitivity'] = []\n","\n","        for chunk_size in CHUNK_SIZES:\n","            print(f\"    Testing chunk_size={chunk_size}\")\n","            config = {\n","                'chunk_size': chunk_size,\n","                'max_tokens': BASELINE_MAX_TOKENS,\n","                'temperature': BASELINE_TEMPERATURE,\n","                'test_type': 'chunk_size_sensitivity',\n","                'variable': 'chunk_size'\n","            }\n","\n","            result = run_technique_sensitivity_test(processor, test_video, technique, config)\n","            all_results['tests'][technique]['chunk_size_sensitivity'].append(result)\n","\n","        # Test 3: Max Tokens Sensitivity\n","        print(f\"\\n  Max Tokens Sensitivity: {MAX_TOKENS_OPTIONS}\")\n","        all_results['tests'][technique]['max_tokens_sensitivity'] = []\n","\n","        for max_tokens in MAX_TOKENS_OPTIONS:\n","            print(f\"    Testing max_tokens={max_tokens}\")\n","            config = {\n","                'chunk_size': BASELINE_CHUNK_SIZE,\n","                'max_tokens': max_tokens,\n","                'temperature': BASELINE_TEMPERATURE,\n","                'test_type': 'max_tokens_sensitivity',\n","                'variable': 'max_tokens'\n","            }\n","\n","            result = run_technique_sensitivity_test(processor, test_video, technique, config)\n","            all_results['tests'][technique]['max_tokens_sensitivity'].append(result)\n","\n","        # Test 4: Temperature Sensitivity\n","        print(f\"\\n  Temperature Sensitivity: {TEMPERATURE_OPTIONS}\")\n","        all_results['tests'][technique]['temperature_sensitivity'] = []\n","\n","        for temperature in TEMPERATURE_OPTIONS:\n","            print(f\"    Testing temperature={temperature}\")\n","            config = {\n","                'chunk_size': BASELINE_CHUNK_SIZE,\n","                'max_tokens': BASELINE_MAX_TOKENS,\n","                'temperature': temperature,\n","                'test_type': 'temperature_sensitivity',\n","                'variable': 'temperature'\n","            }\n","\n","            result = run_technique_sensitivity_test(processor, test_video, technique, config)\n","            all_results['tests'][technique]['temperature_sensitivity'].append(result)\n","\n","        # Save after each technique\n","        save_results(all_results, save_dir, f\"{model_name}_sensitivity_progress.json\")\n","\n","    return all_results\n","\n","# ========================== MAIN EXECUTION ==========================\n","\n","def main():\n","    \"\"\"Main execution function\"\"\"\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"COMPREHENSIVE SENSITIVITY ANALYSIS - ALL 8 PROMPTING TECHNIQUES\")\n","    print(\"=\"*80)\n","    print(\"\\nTesting ALL techniques from your study:\")\n","    print(\"  1. Zero-Shot\")\n","    print(\"  2. Sequential Prompting\")\n","    print(\"  3. Least-to-Most\")\n","    print(\"  4. ReAct\")\n","    print(\"  5. Chain of Thought\")\n","    print(\"  6. True Iterative\")\n","    print(\"  7. Self-Consistency\")\n","    print(\"  8. Meta-Prompting\")\n","    print(\"\\nFor EACH technique, testing:\")\n","    print(f\"  - Chunk sizes: {CHUNK_SIZES}\")\n","    print(f\"  - Max tokens: {MAX_TOKENS_OPTIONS}\")\n","    print(f\"  - Temperatures: {TEMPERATURE_OPTIONS}\")\n","    print(f\"  - Frame extraction: Every {FRAME_INTERVAL}th frame\")\n","    print(\"=\"*80)\n","\n","    # Create results directory\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    save_dir = os.path.join(RESULT_BASE_DIR, f\"complete_analysis_{timestamp}\")\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    print(f\"\\nResults directory: {save_dir}\")\n","\n","    # Load API keys\n","    print(f\"\\n{'='*80}\")\n","    print(\"LOADING API KEYS\")\n","    print(f\"{'='*80}\")\n","\n","    gpt_key = load_api_key(GPT_KEY_PATH)\n","    gemini_key = load_api_key(GEMINI_KEY_PATH)\n","    claude_key = load_api_key(CLAUDE_KEY_PATH)\n","\n","    if not all([gpt_key, gemini_key, claude_key]):\n","        print(\"\\n✗ Failed to load API keys\")\n","        return\n","\n","    # Discover videos\n","    videos = discover_videos_and_frames(DATA_DIR, FRAME_INTERVAL)\n","\n","    if not videos:\n","        print(\"\\n✗ No videos discovered\")\n","        return\n","\n","    # Initialize processors\n","    print(f\"\\n{'='*80}\")\n","    print(\"INITIALIZING MODEL PROCESSORS\")\n","    print(f\"{'='*80}\")\n","\n","    processors = {\n","        'gpt': GPTProcessor(gpt_key),\n","        'gemini': GeminiProcessor(gemini_key),\n","        'claude': ClaudeProcessor(claude_key)\n","    }\n","\n","    print(\"✓ All processors initialized\")\n","\n","    # Run sensitivity analysis for each model\n","    all_model_results = {}\n","\n","    # You can select which techniques to test (comment out to test all 8)\n","    techniques_to_test = [\n","        \"zero_shot\",\n","        \"sequential\",\n","        \"least_to_most\",\n","        \"react\",\n","        \"cot\"\n","        # \"true_iterative\",\n","        # \"self_consistency\",\n","        # \"meta_prompting\"\n","    ]\n","\n","    print(f\"\\nTesting {len(techniques_to_test)} techniques per model\")\n","\n","    for model_name, processor in processors.items():\n","        try:\n","            print(f\"\\n{'='*80}\")\n","            print(f\"STARTING: {model_name.upper()}\")\n","            print(f\"{'='*80}\")\n","\n","            model_results = run_full_sensitivity_for_model(\n","                model_name, processor, videos, save_dir, techniques_to_test\n","            )\n","            all_model_results[model_name] = model_results\n","\n","            # Save individual model results\n","            save_results(\n","                model_results,\n","                save_dir,\n","                f\"{model_name}_complete_all_techniques.json\"\n","            )\n","\n","            print(f\"\\n✓ Completed {model_name.upper()}\")\n","\n","        except Exception as e:\n","            print(f\"\\n✗ Error in {model_name}: {str(e)}\")\n","            continue\n","\n","    # Generate final report\n","    if all_model_results:\n","        print(f\"\\n{'='*80}\")\n","        print(\"GENERATING FINAL REPORT\")\n","        print(f\"{'='*80}\")\n","\n","        final_report = {\n","            'analysis_summary': {\n","                'date': datetime.now().isoformat(),\n","                'frame_extraction': f\"Every {FRAME_INTERVAL}th frame\",\n","                'baseline_config': {\n","                    'chunk_size': BASELINE_CHUNK_SIZE,\n","                    'max_tokens': BASELINE_MAX_TOKENS,\n","                    'temperature': BASELINE_TEMPERATURE\n","                },\n","                'tested_parameters': {\n","                    'chunk_sizes': CHUNK_SIZES,\n","                    'max_tokens': MAX_TOKENS_OPTIONS,\n","                    'temperatures': TEMPERATURE_OPTIONS\n","                },\n","                'techniques_tested': techniques_to_test\n","            },\n","            'models_tested': list(all_model_results.keys()),\n","            'model_results': all_model_results\n","        }\n","\n","        save_results(final_report, save_dir, \"COMPREHENSIVE_FINAL_REPORT.json\")\n","\n","        print(f\"\\n{'='*80}\")\n","        print(\"ANALYSIS COMPLETE!\")\n","        print(f\"{'='*80}\")\n","        print(f\"\\nAll results saved to: {save_dir}\")\n","        print(\"\\nGenerated files:\")\n","        print(\"  - COMPREHENSIVE_FINAL_REPORT.json\")\n","        for model in all_model_results.keys():\n","            print(f\"  - {model}_complete_all_techniques.json\")\n","\n","    else:\n","        print(\"\\n✗ No results generated\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"oGjDJDlU1f3r","executionInfo":{"status":"ok","timestamp":1762219626473,"user_tz":300,"elapsed":10566674,"user":{"displayName":"opeyemi adeniran","userId":"06352095503427961436"}},"outputId":"f471a23b-e99e-4841-dac5-8fbc58be2821"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","COMPREHENSIVE SENSITIVITY ANALYSIS - ALL 8 PROMPTING TECHNIQUES\n","================================================================================\n","\n","Testing ALL techniques from your study:\n","  1. Zero-Shot\n","  2. Sequential Prompting\n","  3. Least-to-Most\n","  4. ReAct\n","  5. Chain of Thought\n","  6. True Iterative\n","  7. Self-Consistency\n","  8. Meta-Prompting\n","\n","For EACH technique, testing:\n","  - Chunk sizes: [5, 10, 15, 20]\n","  - Max tokens: [2048, 4096, 8192]\n","  - Temperatures: [0.1, 0.5, 1.0]\n","  - Frame extraction: Every 90th frame\n","================================================================================\n","\n","Results directory: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/COMPLETED/PROMPTS/SENSITIVITY_ANALYSIS_COMPLETE/complete_analysis_20251103_222831\n","\n","================================================================================\n","LOADING API KEYS\n","================================================================================\n","✓ Loaded API key from chatgpt.txt\n","✓ Loaded API key from Gemini.txt\n","✓ Loaded API key from claude.txt\n","\n","================================================================================\n","DISCOVERING VIDEOS - EXTRACTING EVERY 90TH FRAME\n","================================================================================\n","Found 11 crime types\n","  Shoplifting_Shoplifting003_x264: 902 → 11 frames\n","  Shoplifting_Shoplifting004_x264: 667 → 8 frames\n","  Fighting_Fighting003_x264: 256 → 3 frames\n","  Fighting_Fighting016_x264: 228 → 3 frames\n","  Shooting_Shooting005_x264: 127 → 2 frames\n","  Shooting_Shooting011_x264: 225 → 3 frames\n","  Stealing_Stealing002_x264: 28 → 1 frames\n","  Stealing_Stealing003_x264: 18 → 1 frames\n","  Explosion_Explosion002_x264: 224 → 3 frames\n","  Explosion_Explosion026_x264: 259 → 3 frames\n","  Arson_Arson002_x264: 268 → 3 frames\n","  Arson_Arson003_x264: 268 → 3 frames\n","  Vandalism_Vandalism001_x264: 94 → 2 frames\n","  Vandalism_Vandalism004_x264: 242 → 3 frames\n","  Abuse_Abuse003_x264: 325 → 4 frames\n","  Abuse_Abuse012_x264: 448 → 5 frames\n","  Robbery_Robbery005_x264: 127 → 2 frames\n","  Robbery_Robbery010_x264: 260 → 3 frames\n","  Burglary_Burglary001_x264: 266 → 3 frames\n","  Burglary_Burglary008_x264: 132 → 2 frames\n","  Assault_Assault001_x264: 132 → 2 frames\n","  Assault_Assault002_x264: 218 → 3 frames\n","\n","Total videos: 22\n","\n","================================================================================\n","INITIALIZING MODEL PROCESSORS\n","================================================================================\n","✓ All processors initialized\n","\n","Testing 5 techniques per model\n","\n","================================================================================\n","STARTING: GPT\n","================================================================================\n","\n","================================================================================\n","COMPREHENSIVE SENSITIVITY ANALYSIS FOR GPT\n","Testing 5 prompting techniques\n","================================================================================\n","\n","Test video: Shoplifting_Shoplifting003_x264\n","Frames: 11 (from 902)\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: ZERO_SHOT\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Chunk 1/3\n","      Chunk 2/3\n","      Chunk 3/3\n","    Testing chunk_size=10\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing chunk_size=15\n","      Chunk 1/1\n","    Testing chunk_size=20\n","      Chunk 1/1\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=4096\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=8192\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=0.5\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=1.0\n","      Chunk 1/2\n","      Chunk 2/2\n","  ✓ Saved: gpt_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: SEQUENTIAL\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing chunk_size=10\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing chunk_size=15\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing chunk_size=20\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing max_tokens=4096\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing max_tokens=8192\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing temperature=0.5\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing temperature=1.0\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","  ✓ Saved: gpt_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: LEAST_TO_MOST\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing chunk_size=10\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing chunk_size=15\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing chunk_size=20\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing max_tokens=4096\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing max_tokens=8192\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing temperature=0.5\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing temperature=1.0\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","  ✓ Saved: gpt_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: REACT\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Chunk 1/3\n","      Chunk 2/3\n","      Chunk 3/3\n","    Testing chunk_size=10\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing chunk_size=15\n","      Chunk 1/1\n","    Testing chunk_size=20\n","      Chunk 1/1\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=4096\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=8192\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=0.5\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=1.0\n","      Chunk 1/2\n","      Chunk 2/2\n","  ✓ Saved: gpt_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: COT\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Chunk 1/3\n","      Chunk 2/3\n","      Chunk 3/3\n","    Testing chunk_size=10\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing chunk_size=15\n","      Chunk 1/1\n","    Testing chunk_size=20\n","      Chunk 1/1\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=4096\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=8192\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=0.5\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=1.0\n","      Chunk 1/2\n","      Chunk 2/2\n","  ✓ Saved: gpt_sensitivity_progress.json\n","  ✓ Saved: gpt_complete_all_techniques.json\n","\n","✓ Completed GPT\n","\n","================================================================================\n","STARTING: GEMINI\n","================================================================================\n","\n","================================================================================\n","COMPREHENSIVE SENSITIVITY ANALYSIS FOR GEMINI\n","Testing 5 prompting techniques\n","================================================================================\n","\n","Test video: Shoplifting_Shoplifting003_x264\n","Frames: 11 (from 902)\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: ZERO_SHOT\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1966.96ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.83ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Chunk 1/3\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 718.32ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/3\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 720.84ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 3/3\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.68ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=10\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 963.33ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.51ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=15\n","      Chunk 1/1\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1304.79ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=20\n","      Chunk 1/1\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1011.38ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 930.85ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.59ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=4096\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 779.85ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.87ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=8192\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1108.55ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 484.50ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1072.92ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 307.47ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=0.5\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 855.56ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.77ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=1.0\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 802.69ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.50ms\n"]},{"output_type":"stream","name":"stdout","text":["  ✓ Saved: gemini_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: SEQUENTIAL\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 998.54ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.53ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 953.69ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.53ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 830.98ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.01ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1006.13ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.85ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 778.02ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.26ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 514.85ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 517.79ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.22ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 565.74ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 620.11ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.02ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 489.81ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 518.29ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 333.46ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 518.11ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 568.42ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 337.09ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 464.70ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 492.56ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 335.77ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=10\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 740.28ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 307.98ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 927.03ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.88ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 679.35ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 333.50ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 730.19ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 306.62ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 704.06ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 307.94ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=15\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 703.52ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 784.32ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 781.93ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 870.69ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 830.62ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=20\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1024.96ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 806.01ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 680.66ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 754.88ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 732.02ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 970.06ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 306.98ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 728.30ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 308.93ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 702.75ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 407.63ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 815.85ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 333.59ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 931.22ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 334.72ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=4096\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 676.91ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 309.44ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 728.67ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.92ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 916.75ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.63ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1033.17ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.14ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 879.22ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 433.45ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=8192\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1020.81ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.25ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 776.69ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 386.89ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 853.93ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.20ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 915.16ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 509.14ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 853.14ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 460.35ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 753.54ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 281.70ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 703.89ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.13ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 676.31ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 331.86ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 678.11ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 409.70ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 728.22ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.25ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=0.5\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1148.55ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 336.95ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 803.91ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 337.59ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 678.04ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 331.64ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 923.99ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 407.97ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 752.95ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 335.00ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=1.0\n","      Step 1/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 778.70ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 307.08ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 895.00ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.07ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 828.64ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 411.06ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 904.21ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.17ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1131.82ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.46ms\n"]},{"output_type":"stream","name":"stdout","text":["  ✓ Saved: gemini_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: LEAST_TO_MOST\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 967.18ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1091.82ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 853.82ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 360.54ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1360.73ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.95ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 929.26ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.06ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 855.24ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.49ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 960.82ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.20ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 802.18ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 333.53ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1277.48ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.59ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 591.48ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 728.49ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 434.32ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 541.75ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 830.32ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 359.69ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 669.14ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 728.14ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 384.23ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 718.79ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 849.91ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.18ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 878.69ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 880.84ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 385.16ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 543.40ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 777.72ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 307.04ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 565.52ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 744.20ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 331.98ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 543.16ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 568.15ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 306.82ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=10\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 880.95ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 459.20ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 930.02ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.20ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1044.58ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.13ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1006.15ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 410.84ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 829.17ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 408.30ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 942.34ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.11ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 930.23ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.25ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 854.04ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 307.15ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=15\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 958.04ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 991.29ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1057.88ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1387.19ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1234.48ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 959.64ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1125.47ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1132.00ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=20\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1159.66ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1109.50ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 982.52ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 982.69ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 881.78ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1226.88ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 957.14ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1207.50ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 803.23ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 306.64ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1154.44ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 306.66ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 853.03ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 362.55ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 929.71ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.71ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1236.89ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.56ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1076.31ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.28ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 907.03ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 436.46ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1184.88ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.43ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=4096\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1199.89ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.96ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 905.29ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 361.42ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 882.58ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 359.63ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 878.71ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.93ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1008.34ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 306.77ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 754.44ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 334.13ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 905.60ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.58ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1193.59ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.91ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=8192\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1106.61ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.84ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 929.07ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.52ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 955.67ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.08ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 882.01ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.39ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 807.04ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 360.92ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 907.24ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.34ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1146.05ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 333.46ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 831.27ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 335.01ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 957.79ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.00ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1071.98ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.81ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 753.28ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 334.72ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 803.28ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 333.35ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 855.29ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.28ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 805.99ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 434.48ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 804.61ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 584.75ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 932.59ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 408.36ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=0.5\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1022.67ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 564.10ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 958.20ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 387.02ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 880.32ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.52ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 878.70ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 407.74ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 929.35ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.73ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 957.09ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 333.52ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 803.11ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 308.40ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1075.41ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.44ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=1.0\n","      Step 1/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 884.28ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 385.75ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 2/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 827.91ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 358.32ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 3/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1072.80ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.48ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 4/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 906.13ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 360.74ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 5/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 854.86ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.20ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 6/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 855.74ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 357.75ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 7/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 838.67ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 307.26ms\n"]},{"output_type":"stream","name":"stdout","text":["      Step 8/8\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 879.60ms\n","WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 333.94ms\n"]},{"output_type":"stream","name":"stdout","text":["  ✓ Saved: gemini_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: REACT\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 840.72ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 433.55ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Chunk 1/3\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 541.90ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/3\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 544.93ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 3/3\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 334.04ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=10\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 941.70ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.15ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=15\n","      Chunk 1/1\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 956.92ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=20\n","      Chunk 1/1\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1101.59ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 929.55ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 361.77ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=4096\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 880.85ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 332.23ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=8192\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 879.86ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.74ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 881.54ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 334.02ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=0.5\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 854.74ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.73ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=1.0\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 997.17ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 383.28ms\n"]},{"output_type":"stream","name":"stdout","text":["  ✓ Saved: gemini_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: COT\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 829.40ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 283.73ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Chunk 1/3\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 517.62ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/3\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 517.66ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 3/3\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 336.15ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=10\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3003.55ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 382.82ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=15\n","      Chunk 1/1\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 857.11ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing chunk_size=20\n","      Chunk 1/1\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3767.06ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1891.84ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 5776.29ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=4096\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 7893.30ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 434.12ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing max_tokens=8192\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1871.41ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 459.27ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1485.87ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 333.99ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=0.5\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1132.58ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 306.49ms\n"]},{"output_type":"stream","name":"stdout","text":["    Testing temperature=1.0\n","      Chunk 1/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 803.51ms\n"]},{"output_type":"stream","name":"stdout","text":["      Chunk 2/2\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 281.51ms\n"]},{"output_type":"stream","name":"stdout","text":["  ✓ Saved: gemini_sensitivity_progress.json\n","  ✓ Saved: gemini_complete_all_techniques.json\n","\n","✓ Completed GEMINI\n","\n","================================================================================\n","STARTING: CLAUDE\n","================================================================================\n","\n","================================================================================\n","COMPREHENSIVE SENSITIVITY ANALYSIS FOR CLAUDE\n","Testing 5 prompting techniques\n","================================================================================\n","\n","Test video: Shoplifting_Shoplifting003_x264\n","Frames: 11 (from 902)\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: ZERO_SHOT\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Chunk 1/3\n","      Chunk 2/3\n","      Chunk 3/3\n","    Testing chunk_size=10\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing chunk_size=15\n","      Chunk 1/1\n","    Testing chunk_size=20\n","      Chunk 1/1\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=4096\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=8192\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=0.5\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=1.0\n","      Chunk 1/2\n","      Chunk 2/2\n","  ✓ Saved: claude_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: SEQUENTIAL\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing chunk_size=10\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing chunk_size=15\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing chunk_size=20\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing max_tokens=4096\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing max_tokens=8192\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing temperature=0.5\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","    Testing temperature=1.0\n","      Step 1/5\n","      Step 2/5\n","      Step 3/5\n","      Step 4/5\n","      Step 5/5\n","  ✓ Saved: claude_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: LEAST_TO_MOST\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing chunk_size=10\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing chunk_size=15\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing chunk_size=20\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing max_tokens=4096\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing max_tokens=8192\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing temperature=0.5\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","    Testing temperature=1.0\n","      Step 1/8\n","      Step 2/8\n","      Step 3/8\n","      Step 4/8\n","      Step 5/8\n","      Step 6/8\n","      Step 7/8\n","      Step 8/8\n","  ✓ Saved: claude_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: REACT\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Chunk 1/3\n","      Chunk 2/3\n","      Chunk 3/3\n","    Testing chunk_size=10\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing chunk_size=15\n","      Chunk 1/1\n","    Testing chunk_size=20\n","      Chunk 1/1\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=4096\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=8192\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=0.5\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=1.0\n","      Chunk 1/2\n","      Chunk 2/2\n","  ✓ Saved: claude_sensitivity_progress.json\n","\n","--------------------------------------------------------------------------------\n","TESTING TECHNIQUE: COT\n","--------------------------------------------------------------------------------\n","\n","  Baseline Configuration:\n","    Chunk: 10, Tokens: 4096, Temp: 0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Chunk Size Sensitivity: [5, 10, 15, 20]\n","    Testing chunk_size=5\n","      Chunk 1/3\n","      Chunk 2/3\n","      Chunk 3/3\n","    Testing chunk_size=10\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing chunk_size=15\n","      Chunk 1/1\n","    Testing chunk_size=20\n","      Chunk 1/1\n","\n","  Max Tokens Sensitivity: [2048, 4096, 8192]\n","    Testing max_tokens=2048\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=4096\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing max_tokens=8192\n","      Chunk 1/2\n","      Chunk 2/2\n","\n","  Temperature Sensitivity: [0.1, 0.5, 1.0]\n","    Testing temperature=0.1\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=0.5\n","      Chunk 1/2\n","      Chunk 2/2\n","    Testing temperature=1.0\n","      Chunk 1/2\n","      Chunk 2/2\n","  ✓ Saved: claude_sensitivity_progress.json\n","  ✓ Saved: claude_complete_all_techniques.json\n","\n","✓ Completed CLAUDE\n","\n","================================================================================\n","GENERATING FINAL REPORT\n","================================================================================\n","  ✓ Saved: COMPREHENSIVE_FINAL_REPORT.json\n","\n","================================================================================\n","ANALYSIS COMPLETE!\n","================================================================================\n","\n","All results saved to: /content/drive/Shareddrives/DR KOFI RESEARCH/RESEARCH/COMPLETED/PROMPTS/SENSITIVITY_ANALYSIS_COMPLETE/complete_analysis_20251103_222831\n","\n","Generated files:\n","  - COMPREHENSIVE_FINAL_REPORT.json\n","  - gpt_complete_all_techniques.json\n","  - gemini_complete_all_techniques.json\n","  - claude_complete_all_techniques.json\n"]}]}]}